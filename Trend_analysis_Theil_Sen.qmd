---
title: "ECAN PM trend analysis"
author: "Ben Scott"
date: "`r Sys.Date()`"
format:
  html:
    title: index
    self-contained: true
    toc: true
    output-file: index.html
code-fold: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse, warn.conflicts = FALSE)
library(lubridate)
library(httr)
library(DBI)

# Need a function to round like excel e.g. Function to always round 0.5 up
round2 <- function(x, digits = 0) {
  posneg <- sign(x)
  z <- abs(x) * 10^digits
  z <- z + 0.5
  z <- trunc(z)
  z <- z / 10^digits
  z * posneg
}

```

## Introduction

This is an rmarkdown document showing how to calculate 10 year trends calculated using Environment Canterbury data. This is a working document and will update and change.

The steps shown are from getting data, cleaning the data (with some decisions on analysis) and finally running the trend calculation. The trends are calculated using the `TheilSen` function from the [openair]{.pkg} package.

## Data retrieval

This method uses a stored procedure (`pAir_MonDailyAllParams`) to retrieve **daily averages** from the [ecan website](https://data.ecan.govt.nz/Catalogue/Method?MethodId=98#tab-data). The averages are calculated by a stored procedure `dbo.XXsomethingXX` and stored within the Envista database. The relevant table in the Envista database is `dbo.Alldaily.`

For this document we can just use the website to retrieve the data in `.csv` format. The inputs are a start date, end date and a station. The dates of interest are for the previous 10 years.

```{r}
from_date <- "1/01/2013"
to_date <- "31/12/2022"

### Number of full years check
# Convert type and add a day to account for days between
to_date_mod <- dmy(to_date) + 1
from_date_mod <- dmy(from_date)
time_length(interval(start = from_date_mod, end = to_date_mod), "years")


```

Need to get a list of all stations available via the website.

::: callout-note
Not all stations will have data available but this is taken care of later by ignoring the return if the httr request returns an http error.
:::

```{r}

# URL for getting list of available stations
response_stations <- httr::GET(
  'http://data.ecan.govt.nz/data/23/Air/Air%20quality%20sites%20monitored/CSV'
  )

# Basic list of stations as .csv file using URL above
station_id_list <- content(
  response_stations,
  encoding = "UTF-8",
  show_col_types = FALSE
  ) |>
  # Convert to data frame
  data.frame() |>  
  mutate(LatestDateTime = lubridate::dmy_hms(LatestDateTime))

# This is the base URL we want to use before adding parameters
url_1 <- "https://data.ecan.govt.nz:443/data/98/Air/"
url_2 <- "Air%20quality%20data%20for%20a%20monitored%20site%20(daily)/CSV?"
base_url = paste0(url_1, url_2)

# Get the date into the right format for URL
from_date_url <- stringr::str_replace_all(from_date,"/","%2F")
to_date_url <- stringr::str_replace_all(to_date,"/","%2F")

```

Now can make the actual request in a loop to retrieve data for all stations which have available data.

```{r}

# Initialise datalist to be populated inside the for loop
datalist = list()   

# For loop to make request of each station
for (ii in station_id_list$SiteNo) {
  
  # Make basic URL string
  URL_string <- paste0(
    base_url, "SiteID=", as.character(ii), "&",
    "StartDate=", as.character(from_date_url), "&",
    "EndDate=", as.character(to_date_url)
    )   
  
  # Encode URL strign as URL
  URL_request <- utils::URLencode(URL_string)   

  # Send URL request message
  response <- httr::GET(URL_request)
  
  # Stop if response code is not OK. Could include other codes in future
  if (response$status_code != 200) {
    next
  }

  # Response for that station received
  dat_raw <- content(
    response,
    encoding = "UTF-8",
    show_col_types = FALSE,
    as = "text"
    )   
  
  # Parse text to data table
  dat <- read.table(
    text = dat_raw,
    sep = ",",
    header = TRUE,
    stringsAsFactors = FALSE
    ) 
  
  # Simplify column names
  names(dat) <- gsub(x = names(dat), pattern = "\\.", replacement = "")  
  
  # Different variables at each station
  dat_longer <- pivot_longer(
    dat,
    !c(DateTime, StationName)
    ) |> 
    mutate(
      value = round2(value, 1)
    )

  # Change date format
  dat_longer$DateTime <- lubridate::ymd(dat_longer$DateTime)   

  # add it tolist
  datalist[[ii]] <- dat_longer   
}

# Combine data into one big table as a data frame
df_final_long = do.call(rbind, datalist)   

# This is handy for checking with database to more significant figures.
options(pillar.sigfig = 7)
```

## Data cleaning and decisions

Now need to get the data in a better state for Theil-Sen analysis. First some reformatting and renaming.

```{r}
# Reformat data
df_daily_pm <- df_final_long |>
  # Select only PM10 and PM2.5
  filter(name %in% c("PM10ugm3", "PM25ugm3")) |>
  # Reformat to wide
  pivot_wider(
    names_from = name,
    values_from = value
    ) |>
  # date column required by openair
  rename(
    date = DateTime,
    PM10 = PM10ugm3,
    PM25 = PM25ugm3,
    site = StationName
    ) |>
  # Add columns for data checking
  mutate(
    month = lubridate::month(date),
    year = lubridate::year(date),
    days_per_month = lubridate::days_in_month(date)
    )

```

Before doing the trend estimate it is a good idea to plot the data to check for holes. The [openair]{.pkg} function `summaryPlot` is handy for this.

```{r}
#| column: page
#| fig-width: 20
#| fig-height: 30

# PM10
p_sum_pm10 <- select(df_daily_pm, c("date", "site", "PM10")) |> 
  # Treat date as.character to make % data capture work for summaryPlot
  mutate(date = as.character(date)) |>
  openair::summaryPlot(
    pollutant = "PM10",
    type = "site",
    fontsize = 30
  )

```

And PM~2.5~

```{r}
#| column: page
#| fig-width: 20
#| fig-height: 30

# PM2.5
p_sum_pm25 <- select(df_daily_pm, c("date", "site", "PM25")) |> 
  # Treat date as.character to make % data capture work for summaryPlot
  mutate(date = as.character(date)) |>
  openair::summaryPlot(
    pollutant = "PM25",
    type = "site",
    fontsize = 30
  )


```
The  summary plots show some decisions need to be made as to which sites to include and which periods should be used. The selection of periods are based on the [trend information sheet](https://boplass2.sharepoint.com/:w:/r/sites/NationalAirQualityGrp/_layouts/15/Doc.aspx?sourcedoc=%7BCD47641A-F0E7-420C-B974-40EF61C84B4D%7D&file=LAWA%20air%20quality%20trends%20guidance%2013022022.docx&_DSL=1&action=default&mobileredirect=true). The big decision point is that recent data should be discarded and trends calculated from the period prior if there are large holes (> 5 months) in the data.

The following is a list of the decisions made. The code should reflect these decisions as well.

- No trend should be calculated for Washdyke as the site has been changed and there has been a significant change in concentrations and occurrence of exceedances so we can remove those Washdyke stations from the data set.
- For St Albans, the site was shut down on 2020-11-11 so that will be the end date for this trend analysis for St Albans.

```{r}

df_daily_pm_clean <- df_daily_pm |>
  # Remove Washdyke sites
  filter(!(site %in% c("Washdyke Alpine", "Washdyke Flat Road"))) |> 
  # Remove St Albans data prior to this date due to site movement
  filter(!(date > as.Date("2020-11-11") & site == "St Albans"))

```

## Theil-Sen analysis

So now we can run the Theil-Sen trend analysis on the PM~10~ and PM~2.5~ datasets

```{r}
#| fig-width: 20
#| fig-height: 20
#| message: false
#| warning: false

# PM10 trend analysis
p_pm10 <- openair::TheilSen(
  df_daily_pm_clean,
  pollutant = "PM10",
  deseason = TRUE,
  date.format = "%Y",
  type = "site",
  data.thresh = 75,
  fontsize = 30
  )

p_pm25 <- openair::TheilSen(
  df_daily_pm_clean,
  pollutant = "PM25",
  deseason = TRUE,
  date.format = "%Y",
  type = "site",
  data.thresh = 75,
  fontsize = 30
  )

```

## Trend analysis using `.csv` file

We can also run the same analysis using data retrieved from a .csv file. This is handy for comparison.

::: callout-note
Note the dataset dates need to match if comparing .csv read and retrievals from the website
:::

### Get the data

This assumes the data has been prepared in a .csv file which is included in this [GitHub](https://github.com/bws200/theil-sen-trend-analysis-ecan) repository under `/data`. It is for Environment Canterbury's Rangiora site only. A larger .csv file could be created which includes site name as an additional column.

```{r}
# Read file
df_csv <- read.csv(
  file = "data/Rangiora PM10 2013 to 2022.csv"
) |>
  rename(
    date = DateTime,
    PM10 = PM10..ug.m3.,
    PM25 = PM2.5..ug.m3.
  ) |>
  mutate(date = lubridate::dmy(date))

df_csv |> 
  # Add as.character to make data %'s work ok
  mutate(date = as.character(date)) |> 
  openair::summaryPlot(
    pollutant = "PM10"
  )

head(df_csv)

# Run the trend analysis
openair::TheilSen(
  df_csv,
  pollutant = "PM10",
  deseason = TRUE,
  date.format = "%Y",
  data.thresh = 75
)


```

