---
title: "ECAN PM trend analysis"
author: "Ben Scott"
date: "`r Sys.Date()`"
format:
  html:
    self-contained: true
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(httr)
library(DBI)

# Need a function to round like excel e.g. Function to always round 0.5 up
round2 <- function(x, digits = 0) {
  posneg <- sign(x)
  z <- abs(x) * 10^digits
  z <- z + 0.5
  z <- trunc(z)
  z <- z / 10^digits
  z * posneg
}

```

An rmarkdown document detailing steps to calculate 10 year trends calculated using Environment Canterbury data.

This is a working document and will update and change. The steps shown are from getting data through the whole analysis. It's easiest to do the whole retrieve and analyse data here as each data set has its own quirks.

## Method 1 - using method 98 on data.ecan.govt.nz [link](https://data.ecan.govt.nz/Catalogue/Method?MethodId=98#tab-data) DAILY

This method uses a stored procedure to retrieve daily averages from the [ecan website](https://data.ecan.govt.nz/Catalogue/Method?MethodId=98#tab-data) calculated by and stored within the Envista database. The equivalent table is `dbo.Alldaily.`

The stored procedure `pAir_MonDailyAllParams` takes care of this and outputs data via the website. For here we can just use the website which retrieves the data in `.csv` format. The inputs are a start and end date and a station. This can be automated to request data for each station between dates. The dates of interest are for the previous 10 years

```{r}

from_date <- "01/01/2013"
to_date <- "31/12/2015"

```

The first steps are some setup work for values to get and addresses etc.

```{r}

# URL for getting list of available stations
response_stations <- httr::GET(
  'http://data.ecan.govt.nz/data/23/Air/Air%20quality%20sites%20monitored/CSV'
  )

# Basic list of stations as .csv file using URL above
station_id_list <- content(
  response_stations,
  encoding = "UTF-8",
  show_col_types = FALSE
  ) |>
  # Convert to data frame
  data.frame() |>  
  mutate(LatestDateTime = lubridate::dmy_hms(LatestDateTime)) |>
  # Makes sure date is ok and ignores station if there is no data available
  filter(LatestDateTime >= lubridate::dmy(from_date))   

# This is the base URL we want to use before adding parameters
base_url = "https://data.ecan.govt.nz:443/data/98/Air/Air%20quality%20data%20for%20a%20monitored%20site%20(daily)/CSV?"

# Get the date into the right format for URL
from_date_url <- stringr::str_replace_all(from_date,"/","%2F")
to_date_url <- stringr::str_replace_all(to_date,"/","%2F")

```

Now can make a request in a loop for all the stations available

```{r}

# Initialise datalist to be populated inside the for loop
datalist = list()   

# For loop to make request of each station
for (ii in station_id_list$SiteNo) {
  
  # Make basic URL string
  URL_string <- paste0(
    base_url, "SiteID=", as.character(ii), "&",
    "StartDate=", as.character(from_date_url), "&",
    "EndDate=", as.character(to_date_url)
    )   
  
  # Encode URL strign as URL
  URL_request <- utils::URLencode(URL_string)   

  # Send URL request message
  response <- httr::GET(URL_request)
  
  # Stop if response code is an error. Could include other codes in future
  if (response$status_code == 500) {
    next
  }

  # Response for that station received
  dat_raw <- content(
    response,
    encoding = "UTF-8",
    show_col_types = FALSE,
    as = "text"
    )   
  
  # Parse text to data table
  dat <- read.table(
    text = dat_raw,
    sep = ",",
    header = TRUE,
    stringsAsFactors = FALSE
    ) 
  
  # Simplify column names
  names(dat) <- gsub(x = names(dat), pattern = "\\.", replacement = "")  
  
  # Different variables at each station
  dat_longer <- pivot_longer(
    dat,
    !c(DateTime, StationName)
    ) |> 
    mutate(
      value = round2(value, 1)
    )

  # Change date format
  dat_longer$DateTime <- lubridate::ymd(dat_longer$DateTime)   

  # add it tolist
  datalist[[ii]] <- dat_longer   
}

# Combine data into one big table as a data frame
df_final_long = do.call(rbind, datalist)   

# This is handy for checking with database to more significant figures.
options(pillar.sigfig = 7)
```

Now need to get the data ready for TheilSen analysis

```{r}
# Reformat data
df_daily_pm <- df_final_long |>
  filter(name %in% c("PM10ugm3", "PM25ugm3")) |>
  # Reformat to wide
  pivot_wider(
    names_from = name,
    values_from = value
    ) |>
  # date column required by openair
  rename(
    date = DateTime,
    PM10 = PM10ugm3,
    PM25 = PM25ugm3,
    site = StationName
    ) |>
  # Add columns for data checking
  mutate(
    month = lubridate::month(date),
    year = lubridate::year(date),
    days_per_month = lubridate::days_in_month(date)
    )

# Rename stations to combine them - NOT best practice
df_daily_pm$site <- recode(
  df_daily_pm$site,
  "St Albans EP" = "St Albans",
  "Waimate Kennedy" = "Waimate",
  "Waimate Stadium" = "Waimate"
  )

```

Before doing the trend estimate it is a good idea to plot the data to check for holes. The [openair]{.pkg} function `summaryPlot` is handy for this

```{r}
#| column: page
#| fig-width: 20
#| fig-height: 30

df_daily_pm |>
  select(!PM25) |> 
  group_by(site, year, month) |>
  summarise(n = n()/mean(days_per_month) * 100) |> 
  filter(n < 75)

openair::summaryPlot(
  mydata = select(df_daily_pm, c("date", "site", "PM10", "PM25")),
  pollutant = c("PM10"),
  fontsize = 30
)

openair::summaryPlot(
  mydata = select(df_daily_pm, c("date", "site", "PM10", "PM25")),
  pollutant = c("PM25"),
  fontsize = 30
)

```

No trend should be calculated for Washdyke as the site has been changed so we can remove those stations from the data set.

```{r}
# Create PM10 dataset without Washdyke
df_daily_pm_no_washdyke <- df_daily_pm |>
  # Remove Washdyke sites
  filter(!(site %in% c("Washdyke Alpine", "Washdyke Flat Road"))) |> 
  # Remove PM2.5
  select(!c(PM25)) |>
  # Create dummy variable for removing data later
  mutate(dummy = paste0(site,month,year))

# Create a check dataframe which shows 
df_check <- df_daily_pm_no_washdyke |> 
  group_by(site, year, month) |>
  count(Logic = !is.na(PM10)) |> 
  filter(Logic == FALSE & n >= 7) |>  # 7 days in a month missing at most
  # Create dummy variable
  mutate(dummy2 = paste0(site, month, year)) 

# 
# openair::summaryPlot(
#   mydata = filter(
#     df_daily_pm_no_washdyke,
#     !(dummy %in% df_check$dummy2)),
#   fontsize = 30
# )

```

This suggests that all remaining sites are OK, other than St Albans data. Based on the trend information sheet, recent data should be discarded and trends calculated from the period prior to 2020-11. The site was shut down then 2020-11-11 so that will be the end date.

```{r}
#| column: page
#| fig-width: 20
#| fig-height: 30

df_daily_pm10_clean <- df_daily_pm_no_washdyke |>
  # Remove St Albans data prior to this date due to site movement
  filter(!(date > as.Date("2020-11-11") & site == "St Albans"))

# Summary plot
openair::summaryPlot(
  mydata = filter(
    df_daily_pm10_clean,
    !(dummy %in% df_check$dummy2)),
  fontsize = 30
)


```

So now can run the Theil-Sen analysis

```{r}
#| fig-width: 20
#| echo: false
#| message: false

# PM10 trend analysis
p_pm10 <- openair::TheilSen(
  df_daily_pm10_clean,
  pollutant = "PM10",
  deseason = TRUE,
  date.format = "%Y",
  type = "site",
  data.thresh = 75,
  fontsize = 30
  )

```

:::


Now can run the Theil-Sen analysis using the function `TheiSen` from the [openair]{.pkg}

```{r}
# 
# # PM10 trend analysis
# p_pm10 <- openair::TheilSen(
#   df_daily_pm,
#   pollutant = "PM10",
#   deseason = TRUE,
#   date.format = "%Y",
#   type = "site"
#   )
# 
# # PM2.5 trend analysis
# 
# p_pm25 <- openair::TheilSen(
#   df_daily_pm,
#   pollutant = "PM25",
#   deseason = TRUE,
#   date.format = "%Y",
#   type = "site"
#   )

```

## Method 2 - retrieve data from a .csv file

```{r}

df_csv <- read.csv(
  file = "data/Rangiora PM10 2013 to 2022.csv"
) |>
  rename(
    date = DateTime,
    PM10 = PM10..ug.m3.,
    PM2.5 = PM2.5..ug.m3.
  ) |>
  mutate(date = lubridate::dmy(date))

p_csv <- openair::TheilSen(
  df_csv,
  pollutant = "PM10",
  deseason = TRUE,
  date.format = "%Y",
  data.thresh = 75
)
# 

```


## Generate time series data to test Theil-Sen function

```{r}

# 
# from_date2 <- lubridate::dmy(from_date)
# to_date2 <- lubridate::dmy(to_date)
# 
# freq <- 1/365   # Not sure how to deal with leap years?
# 
# x_days <- seq(from_date2, to_date2, by = "days")
# 
# x <- seq(1, length(x_days))
# y <- -10 * (sin(x * freq * 2 * pi + 0.5 / freq * pi) - 2) + -0.005 * x + 50
# 
# zoo_object_test <- data.frame(x_days,y) |> 
#   rename(date = x_days) |> 
#   mutate(
#     year = lubridate::year(date),
#     month = lubridate::month(date),
#     day = lubridate::day(date),
#     days_in_month = lubridate::days_in_month(month)
#   ) |> 
#   mutate(
#     # Second case when missing a whole year of data
#     y2 = case_when(
#       year == 2018 ~ NA_real_,
#       TRUE ~ y
#       ),
#     # Third case when missing the whole winter for 4 years of the dataset
#     y3 = case_when(
#       year %in% c(2015, 2017, 2019, 2021) & month %in% c(5, 6, 7, 8, 9) ~ NA_real_,
#       TRUE ~ y
#     ),
#     # Placeholder case where 1 day is missing from each month
#     y4 = case_when(
#       day %in% c(1) ~ NA_real_,
#       TRUE ~ y
#     )
#     )
# 
# 
# # y2 - case where 1 year is missing 6 months of data
# # zoo_object_test$y2[(zoo_object_test$year == 2018)] <- NA
# 
# plot(zoo_object_test$date, zoo_object_test$y3, col = "red")
# 
# # zoo_object_test$y[zoo_object_test$day < 5] <- NA   # First 4 days of every month missing
# zoo_object_test |> 
#   group_by(year, month) |> 
#   summarise(
#     mean_y = mean(y3, na.rm = TRUE),
#     `valid_%` = round(sum((!is.na(y3)) / days_in_month) * 100, 1),
#     days_in_month_total = mean(days_in_month)
# ) |> 
#   print()
# 
# 
# plot(zoo_object_test$date, zoo_object_test$y2)
# 
# zoo_object_test |> 
#   openair::TheilSen(
#     pollutant = "y",
#     deseason = FALSE,
#     date.format = "%Y"
#   )



```





