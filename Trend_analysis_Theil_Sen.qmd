---
title: "ECAN PM trend analysis"
author: "Ben Scott"
date: "`r Sys.Date()`"
repo-url: "https://github.com/bws200/theil-sen-trend-analysis-ecan"
repo-actions: [edit]
format:
  html:
    self-contained: true
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(httr)
library(DBI)

# Need a function to round like excel e.g. Function to always round 0.5 up
round2 <- function(x, digits = 0) {
  posneg <- sign(x)
  z <- abs(x) * 10^digits
  z <- z + 0.5
  z <- trunc(z)
  z <- z / 10^digits
  z * posneg
}

```

## Introduction

This is an rmarkdown document showing how to calculate 10 year trends calculated using Environment Canterbury data. This is a working document and will update and change.

The steps shown are from getting data, cleaning the data (with some decisions on analysis) and finally running the trend calculation. The trends are calculated using the `TheilSen` function from the [openair]{.pkg} package.

## Data retrieval

This method uses a stored procedure (`pAir_MonDailyAllParams`) to retrieve $daily averages$ from the [ecan website](https://data.ecan.govt.nz/Catalogue/Method?MethodId=98#tab-data). The averages are calculated by a stored procedure `dbo.XXsomethingXX` and stored within the Envista database. The relevant table in the Envista database is `dbo.Alldaily.`

For this document we can just use the website to retrieve the data in `.csv` format. The inputs are a start date, end date and a station. The dates of interest are for the previous 10 years.

```{r}
from_date <- "01/01/2013"
to_date <- "31/12/2022"
```

Need to get a list of all stations available via the website.

::: callout-note
Not all stations will have data available but this is taken care of later by ignoring the return if the httr request returns an http error.
:::

```{r}

# URL for getting list of available stations
response_stations <- httr::GET(
  'http://data.ecan.govt.nz/data/23/Air/Air%20quality%20sites%20monitored/CSV'
  )

# Basic list of stations as .csv file using URL above
station_id_list <- content(
  response_stations,
  encoding = "UTF-8",
  show_col_types = FALSE
  ) |>
  # Convert to data frame
  data.frame() |>  
  mutate(LatestDateTime = lubridate::dmy_hms(LatestDateTime))



# This is the base URL we want to use before adding parameters
base_url = "https://data.ecan.govt.nz:443/data/98/Air/Air%20quality%20data%20for%20a%20monitored%20site%20(daily)/CSV?"

# Get the date into the right format for URL
from_date_url <- stringr::str_replace_all(from_date,"/","%2F")
to_date_url <- stringr::str_replace_all(to_date,"/","%2F")

```

Now can make the actual request in a loop to retrieve data for all stations which have available data.

```{r}

# Initialise datalist to be populated inside the for loop
datalist = list()   

# For loop to make request of each station
for (ii in station_id_list$SiteNo) {
  
  # Make basic URL string
  URL_string <- paste0(
    base_url, "SiteID=", as.character(ii), "&",
    "StartDate=", as.character(from_date_url), "&",
    "EndDate=", as.character(to_date_url)
    )   
  
  # Encode URL strign as URL
  URL_request <- utils::URLencode(URL_string)   

  # Send URL request message
  response <- httr::GET(URL_request)
  
  # Stop if response code is not OK. Could include other codes in future
  if (response$status_code != 200) {
    next
  }

  # Response for that station received
  dat_raw <- content(
    response,
    encoding = "UTF-8",
    show_col_types = FALSE,
    as = "text"
    )   
  
  # Parse text to data table
  dat <- read.table(
    text = dat_raw,
    sep = ",",
    header = TRUE,
    stringsAsFactors = FALSE
    ) 
  
  # Simplify column names
  names(dat) <- gsub(x = names(dat), pattern = "\\.", replacement = "")  
  
  # Different variables at each station
  dat_longer <- pivot_longer(
    dat,
    !c(DateTime, StationName)
    ) |> 
    mutate(
      value = round2(value, 1)
    )

  # Change date format
  dat_longer$DateTime <- lubridate::ymd(dat_longer$DateTime)   

  # add it tolist
  datalist[[ii]] <- dat_longer   
}

# Combine data into one big table as a data frame
df_final_long = do.call(rbind, datalist)   

# This is handy for checking with database to more significant figures.
options(pillar.sigfig = 7)
```

## Data cleaning and decisions

Now need to get the data in a better state for TheilSen analysis. First some reformatting and renaming.

```{r}
# Reformat data
df_daily_pm <- df_final_long |>
  filter(name %in% c("PM10ugm3", "PM25ugm3")) |>
  # Reformat to wide
  pivot_wider(
    names_from = name,
    values_from = value
    ) |>
  # date column required by openair
  rename(
    date = DateTime,
    PM10 = PM10ugm3,
    PM25 = PM25ugm3,
    site = StationName
    ) |>
  # Add columns for data checking
  mutate(
    month = lubridate::month(date),
    year = lubridate::year(date),
    days_per_month = lubridate::days_in_month(date)
    )

```

Now we combine some stations. This is not best practice but collocations were carried out at these sites.

```{r}
# Rename stations to combine them - NOT best practice
df_daily_pm$site <- recode(
  df_daily_pm$site,
  "St Albans EP" = "St Albans",
  "Waimate Kennedy" = "Waimate",
  "Waimate Stadium" = "Waimate"
  )

```

Before doing the trend estimate it is a good idea to plot the data to check for holes. The [openair]{.pkg} function `summaryPlot` is handy for this. First we can make the plot for PM~10~.

```{r}
#| column: page
#| fig-width: 20
#| fig-height: 30

test_data <- select(df_daily_pm, c("date", "site", "PM10")) |> 
  # For some reason this displays the correct data capture %
  mutate(date = as.character(date))
  
  # If mutate(date = as.Date(date)) or mutate(date = lubridate::ymd(date)) is used the data capture shows 50%. Not sure why yet

openair::summaryPlot(
  mydata = test_data,
  pollutant = c("PM10"),
  fontsize = 30

)

```

For here, PM~2.5~ will not be looked at but it could be easily done in future. There would need to be some more decisions as there is less PM~2.5~ data available than there is PM~10~.

Can also produce a small summary table to identify % availability of data for each site and if it is less than a certain percentage.

```{r}
df_daily_pm |>
  select(!PM25) |>
  group_by(site, year, month) |>
  summarise(n = n()/mean(days_per_month) * 100) |>
  filter(n < 75)
```

No trend should be calculated for Washdyke as the site has been changed and there has been a significant change in concentrations and occurrence of exceedances so we can remove those Washdyke stations from the data set.

```{r}
# Create PM10 dataset without Washdyke
df_daily_pm_no_washdyke <- df_daily_pm |>
  # Remove Washdyke sites
  filter(!(site %in% c("Washdyke Alpine", "Washdyke Flat Road"))) |> 
  # Remove PM2.5
  select(!c(PM25)) |>
  # Create dummy variable for removing data later
  mutate(dummy = paste0(site,month,year))

```

```{r}
# Create a check dataframe to use later for filtering out invalid data 
df_check <- df_daily_pm_no_washdyke |> 
  group_by(site, year, month) |>
  # Find na values
  count(Logic = !is.na(PM10)) |>
  # 7 days in a month missing at most
  filter(Logic == FALSE & n >= 7) |> 
  # Create dummy variable
  mutate(dummy2 = paste0(site, month, year)) 

```

This suggests that all remaining sites are OK, other than St Albans data. Based on the [trend information sheet](XX%20link%20XX), recent data should be discarded and trends calculated from the period prior to around 2020-11. The site was shut down on 2020-11-11 so that will be the end date for this trend analysis for St Albans

```{r}

df_daily_pm10_clean <- df_daily_pm_no_washdyke |>
  # Remove St Albans data prior to this date due to site movement
  filter(!(date > as.Date("2020-11-11") & site == "St Albans"))

```

## TheilSen analysis

So now we can run the TheilSen trend analysis on the PM~10~ dataset

```{r}
#| column: page
#| fig-width: 20
#| fig-height: 20
#| message: false
#| warning: false

# PM10 trend analysis
p_pm10 <- openair::TheilSen(
  df_daily_pm10_clean,
  pollutant = "PM10",
  deseason = TRUE,
  date.format = "%Y",
  type = "site",
  data.thresh = 75, 
  fontsize = 30
  )

```

## Trend analysis using `.csv` file

We can also run the same analysis using data retrieved from a .csv file. This is handy for comparison.

::: callout-note
Note the dataset dates need to match if comparing .csv read and retrievals from the website
:::

### Get the data

This assumes the data has been prepared in a .csv file which is included in this repo. It is for Ecan's Rangiora site only.

```{r}
# Read file
df_csv <- read.csv(
  file = "data/Rangiora PM10 2013 to 2022.csv"
) |>
  rename(
    date = DateTime,
    PM10 = PM10..ug.m3.,
    PM25 = PM2.5..ug.m3.
  ) |>
  mutate(date = lubridate::dmy(date))

df_csv_summary <- df_csv |> 
  select(c("date", "PM10")) |> 
  mutate(date = as.character(date))

openair::summaryPlot(
  mydata = df_csv_summary
)

# Run the trend analysis
openair::TheilSen(
  df_csv,
  pollutant = "PM10",
  deseason = TRUE,
  date.format = "%Y",
  data.thresh = 75
)

```

There is some bug with the `summaryPlot` function where %'s should be 100% and are displaying as 50%. Converting the date field to character using as.character() seems to be a temporary fix for now.

```{r}
#| echo: false

# 
# from_date2 <- lubridate::dmy(from_date)
# to_date2 <- lubridate::dmy(to_date)
# 
# freq <- 1/365   # Not sure how to deal with leap years?
# 
# x_days <- seq(from_date2, to_date2, by = "days")
# 
# x <- seq(1, length(x_days))
# y <- -10 * (sin(x * freq * 2 * pi + 0.5 / freq * pi) - 2) + -0.005 * x + 50
# 
# zoo_object_test <- data.frame(x_days,y) |> 
#   rename(date = x_days) |> 
#   mutate(
#     year = lubridate::year(date),
#     month = lubridate::month(date),
#     day = lubridate::day(date),
#     days_in_month = lubridate::days_in_month(month)
#   ) |> 
#   mutate(
#     # Second case when missing a whole year of data
#     y2 = case_when(
#       year == 2018 ~ NA_real_,
#       TRUE ~ y
#       ),
#     # Third case when missing the whole winter for 4 years of the dataset
#     y3 = case_when(
#       year %in% c(2015, 2017, 2019, 2021) & month %in% c(5, 6, 7, 8, 9) ~ NA_real_,
#       TRUE ~ y
#     ),
#     # Placeholder case where 1 day is missing from each month
#     y4 = case_when(
#       day %in% c(1) ~ NA_real_,
#       TRUE ~ y
#     )
#     )
# 
# 
# # y2 - case where 1 year is missing 6 months of data
# # zoo_object_test$y2[(zoo_object_test$year == 2018)] <- NA
# 
# plot(zoo_object_test$date, zoo_object_test$y3, col = "red")
# 
# # zoo_object_test$y[zoo_object_test$day < 5] <- NA   # First 4 days of every month missing
# zoo_object_test |> 
#   group_by(year, month) |> 
#   summarise(
#     mean_y = mean(y3, na.rm = TRUE),
#     `valid_%` = round(sum((!is.na(y3)) / days_in_month) * 100, 1),
#     days_in_month_total = mean(days_in_month)
# ) |> 
#   print()
# 
# 
# plot(zoo_object_test$date, zoo_object_test$y2)
# 
# zoo_object_test |> 
#   openair::TheilSen(
#     pollutant = "y",
#     deseason = FALSE,
#     date.format = "%Y"
#   )

```
